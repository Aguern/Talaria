# RunRepeat AI Scraper
## Architecture Modulaire Stealth + IA

Ce syst√®me de scraping utilise une approche moderne qui combine :
- **Playwright** avec mesures stealth maximales pour √©viter les 403
- **BeautifulSoup** pour nettoyer le HTML et r√©duire les co√ªts
- **OpenAI GPT-4o-mini** pour extraire les donn√©es (fini les s√©lecteurs CSS fragiles !)

---

## üì¶ Architecture

```
scraping/
‚îú‚îÄ‚îÄ stealth_browser.py      # Module A: Navigation furtive (Playwright + Stealth)
‚îú‚îÄ‚îÄ html_cleaner.py          # Module B: Nettoyage HTML (BeautifulSoup)
‚îú‚îÄ‚îÄ ai_extractor.py          # Module C: Extraction IA (OpenAI)
‚îú‚îÄ‚îÄ runrepeat_scraper.py     # Pipeline complet
‚îî‚îÄ‚îÄ README_AI_SCRAPER.md     # Ce fichier
```

### Module A: Stealth Browser
- Playwright avec User-Agent r√©aliste
- D√©sactive tous les flags d'automation
- Mouvements de souris et scrolling simul√©s
- D√©lais al√©atoires entre actions
- Fingerprint navigateur r√©aliste

### Module B: HTML Cleaner
- Supprime scripts, styles, images, SVG
- Garde seulement la structure s√©mantique (h1-h6, p, div, table)
- R√©duit le HTML de ~90% (√©conomise tokens OpenAI)
- Fonctions: `clean_html()`, `extract_text_only()`, `get_structured_content()`

### Module C: AI Extractor
- Utilise OpenAI Structured Outputs (garantit JSON valide)
- Sch√©ma Pydantic pour validation
- Extrait: nom, poids, drop, score, pros/cons, specs techniques
- Mode batch pour scraper plusieurs chaussures en parall√®le

---

## üöÄ Installation

### 1. Installer les d√©pendances Python

```bash
cd app/packs/stridematch/scraping

# Installer les packages
pip install playwright playwright-stealth beautifulsoup4 lxml openai pydantic
```

### 2. Installer Playwright browsers

```bash
# Installer Chromium (headless)
playwright install chromium
```

### 3. Configurer l'API OpenAI

Ajouter dans votre `.env` :

```bash
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx
```

---

## üß™ Tests

### Test 1: V√©rifier l'acc√®s √† RunRepeat

```bash
python runrepeat_scraper.py --test
```

**R√©sultat attendu:**
```
‚úÖ ACCESS OK - Fetched 250000 characters
Stealth configuration appears to be working!
```

**Si vous voyez `‚ùå BLOCKED`:**
- Vous √™tes sur un serveur cloud (AWS, GCP, DigitalOcean, etc.)
- RunRepeat d√©tecte l'IP de datacenter
- **Solution:** Lancer depuis votre PC local (IP r√©sidentielle)

---

### Test 2: Tester chaque module ind√©pendamment

#### Module A (Stealth Browser)
```bash
cd app/packs/stridematch/scraping
python stealth_browser.py
```

#### Module B (HTML Cleaner)
```bash
python html_cleaner.py
```

#### Module C (AI Extractor)
```bash
# N√©cessite OPENAI_API_KEY dans .env
python ai_extractor.py
```

---

## üìñ Utilisation

### Scraper une seule chaussure

```bash
python runrepeat_scraper.py https://runrepeat.com/nike-pegasus-41
```

**Sortie:**
```json
[
  {
    "model_name": "Nike Pegasus 41",
    "score": 87.0,
    "weight_g": 280,
    "drop_mm": 10.0,
    "stack_heel_mm": 37.0,
    "stack_forefoot_mm": 27.0,
    "pros": [
      "Comfortable and responsive cushioning",
      "Durable outsole with excellent traction",
      "Great value for daily training"
    ],
    "cons": [
      "Too heavy for racing (280g)",
      "Limited color options"
    ],
    "source_url": "https://runrepeat.com/nike-pegasus-41"
  }
]
```

---

### Scraper plusieurs chaussures

Cr√©er un fichier `urls.txt` :

```
https://runrepeat.com/nike-pegasus-41
https://runrepeat.com/adidas-ultraboost-23
https://runrepeat.com/hoka-clifton-9
https://runrepeat.com/asics-gel-nimbus-26
```

Lancer le scraping batch :

```bash
python runrepeat_scraper.py --urls urls.txt --output results.json
```

**Comportement:**
- Scrape chaque URL s√©quentiellement
- D√©lai de 10 secondes entre chaque requ√™te (politesse)
- Sauvegarde dans `results.json`

---

### Options avanc√©es

#### Sauvegarder le HTML brut et nettoy√© (debug)

```bash
python runrepeat_scraper.py https://runrepeat.com/nike-pegasus-41 --save-raw
```

G√©n√®re :
- `raw_nike-pegasus-41.html` - HTML brut apr√®s Playwright
- `cleaned_nike-pegasus-41.html` - HTML apr√®s nettoyage

#### Personnaliser le fichier de sortie

```bash
python runrepeat_scraper.py --urls urls.txt --output my_data.json
```

---

## üí∞ Co√ªts OpenAI

Le module utilise **GPT-4o-mini** (le mod√®le le moins cher d'OpenAI).

**Estimation:**
- HTML nettoy√©: ~2000-4000 tokens (input)
- R√©ponse JSON: ~500 tokens (output)
- **Co√ªt par chaussure:** ~$0.001 - $0.002 USD (0.1 √† 0.2 centime)

**Pour 100 chaussures:** ~$0.10 - $0.20 USD

üëâ Bien moins cher que de maintenir des s√©lecteurs CSS qui cassent constamment !

---

## ‚ö†Ô∏è Important: Question de l'IP

### ‚ùå Ne fonctionnera PAS depuis :
- Serveurs cloud (AWS, GCP, Azure, DigitalOcean, Hetzner, etc.)
- VPS / machines virtuelles
- **Raison:** RunRepeat d√©tecte les IPs de datacenter et retourne 403

### ‚úÖ Fonctionnera depuis :
- **Votre PC/Mac personnel** (connexion Wifi/fibre maison)
- Serveur avec IP r√©sidentielle (proxy r√©sidentiel)

### Solutions si vous √™tes bloqu√© :

1. **Recommand√©:** Lancer depuis votre ordinateur local
   ```bash
   # Sur votre Mac/PC
   cd ~/Desktop/SaaS_NR/app/packs/stridematch/scraping
   python runrepeat_scraper.py --test
   ```

2. **Alternative:** Utiliser un proxy r√©sidentiel
   - Services: Bright Data, Oxylabs, SmartProxy
   - Co√ªt: ~$5-10 / Go de data
   - Configuration: ajouter proxy dans `stealth_browser.py`

3. **Alternative:** Scraper depuis GitHub Actions
   - Les runners GitHub ont parfois des IPs non-bloqu√©es
   - Gratuit pour repos publics

---

## üîß Personnalisation

### Modifier les champs extraits

√âditer `ai_extractor.py`, classe `ShoeData` :

```python
class ShoeData(BaseModel):
    model_name: str
    weight_g: Optional[int]
    # Ajouter vos champs ici
    heel_counter_stiffness: Optional[str] = None
    breathability_score: Optional[int] = None
```

### Ajuster les mesures stealth

√âditer `stealth_browser.py`, fonction `get_page_content()` :

```python
# Augmenter le d√©lai de simulation humaine
await asyncio.sleep(random.uniform(2.0, 5.0))  # Au lieu de 1.0-2.5

# Augmenter le wait_time
wait_time=10  # Au lieu de 5
```

### Changer le mod√®le OpenAI

```python
# Utiliser GPT-4o (plus cher mais plus pr√©cis)
data = await extract_shoe_data(cleaned, model="gpt-4o")

# Ou Claude 3.5 Sonnet (Anthropic)
# Modifier ai_extractor.py pour utiliser l'API Anthropic
```

---

## üêõ D√©pannage

### Erreur: `403 Forbidden`

**Cause:** IP de datacenter d√©tect√©e

**Solution:**
1. Lancer depuis votre PC local
2. V√©rifier avec `python runrepeat_scraper.py --test`
3. Si toujours bloqu√©, essayer un proxy r√©sidentiel

### Erreur: `OPENAI_API_KEY not found`

**Solution:**
```bash
# Dans votre .env
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx
```

### Erreur: `playwright install chromium`

**Solution:**
```bash
playwright install chromium
playwright install-deps chromium  # Sur Linux
```

### Extraction incompl√®te (champs manquants)

**Cause:** L'IA n'a pas trouv√© les donn√©es dans le HTML

**Solution:**
1. V√©rifier le HTML nettoy√© : `--save-raw`
2. Ajuster le prompt dans `ai_extractor.py`
3. Augmenter `max_tokens` si la r√©ponse est tronqu√©e

---

## üìä Workflow complet

```
1. get_page_content(url)
   ‚Üì
   HTML brut (250KB)
   ‚Üì
2. clean_html(html)
   ‚Üì
   HTML nettoy√© (25KB, -90%)
   ‚Üì
3. extract_shoe_data(cleaned)
   ‚Üì
   JSON structur√©
   ‚Üì
4. Sauvegarder en base de donn√©es
```

---

## üéØ Next Steps

Une fois le scraping fonctionnel :

1. **Int√©grer √† la base de donn√©es PostgreSQL**
   - Adapter le pipeline pour ins√©rer dans `stridematch_products`
   - Mapper les champs AI ‚Üí sch√©ma DB

2. **Automatiser avec Celery**
   - Cr√©er une t√¢che Celery pour scraping nocturne
   - Scheduler avec `celery beat`

3. **Ajouter d'autres sources**
   - Dupliquer la structure pour RunningShoesGuru
   - Adapter le sch√©ma `ShoeData` pour chaque source

4. **Monitoring & Logs**
   - Logger dans PostgreSQL les succ√®s/√©checs
   - Dashboard Grafana pour suivre le scraping

---

## üìù Licence

Utilisation interne uniquement. Respecter les robots.txt et les ToS de RunRepeat.

---

## üôè Cr√©dits

- **Playwright** : https://playwright.dev/
- **OpenAI** : https://openai.com/
- **BeautifulSoup** : https://www.crummy.com/software/BeautifulSoup/
